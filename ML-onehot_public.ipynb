{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4d274-73f3-47ca-84d3-ceea3a713db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proprietary data loading removed.\n",
    "# df = pd.read_csv('proprietary_dataset.csv')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create dummy compound feature matrix\n",
    "np.random.seed(0)\n",
    "fake_df = pd.DataFrame({\n",
    "    'BB1_A': np.random.randint(0, 2, 100),\n",
    "    'BB1_B': np.random.randint(0, 2, 100),\n",
    "    'BB2_C': np.random.randint(0, 2, 100),\n",
    "    'BB3_D': np.random.randint(0, 2, 100),\n",
    "    'fluorescence': np.random.normal(1000, 100, 100)\n",
    "})\n",
    "fake_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9de97fb-e359-456f-96ba-b6077f36d4ee",
   "metadata": {},
   "source": [
    "# Same pre-processing approach as the hit identification method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b58b3-4d54-4145-ba3e-37328e0f2190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# intensity delta difference\n",
    "df['intensity'] = df['81nM'] - df['BL']\n",
    "\n",
    "#Bead_ID to (row, col) in right-to-left in row-major order\n",
    "df['row'] = (df['Bead_ID'] - 1) // 200\n",
    "df['col'] = 199 - ((df['Bead_ID'] - 1) % 200)\n",
    "\n",
    "# smoothing function for nearest neighbors\n",
    "def smooth_by_knn(group, k=8):\n",
    "    coords = group[['row', 'col']].values\n",
    "    intensities = group['intensity'].values\n",
    "    n_points = len(coords)\n",
    "    \n",
    "    if n_points <= 1:\n",
    "        return pd.Series(intensities, index=group.index)\n",
    "    \n",
    "    n_neighbors = min(k + 1, n_points)  # Include self in query\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(coords)\n",
    "    _, indices = nbrs.kneighbors(coords)\n",
    "\n",
    "    #subtract median of neighbors (excluding self)\n",
    "    local_bg = np.array([\n",
    "        np.median(intensities[neigh[1:]]) if len(neigh) > 1 else 0\n",
    "        for neigh in indices\n",
    "    ])\n",
    "    corrected = np.maximum(intensities - local_bg, 0)# prevents negatives by clipping at zero\n",
    "    return pd.Series(corrected, index=group.index)\n",
    "\n",
    "#spatial smoothing per FOV\n",
    "df['spatially_corrected'] = df.groupby('FOV', group_keys=False).apply(smooth_by_knn)\n",
    "\n",
    "df['final_intensity'] = df['spatially_corrected']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed30624e-447b-4621-a274-056f88c49751",
   "metadata": {},
   "source": [
    "# Sampling for a smaller dataset\n",
    "To create a representative subsample of the data, I performed stratified sampling based on the distribution of log_intensity values. First, I divided the data into five quantile-based bins using pd.qcut, ensuring that each bin contained approximately the same number of entries and captured a specific range of log_intensity. Then, I sampled 20% of rows from each bin, ensuring that the resulting sample preserved the overall distribution of intensity values. After sampling, I dropped the bin column and shuffled the rows to remove any ordering bias. This approach ensures that the subsample is both statistically representative and randomized. I also log transformed the final intensity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b84bdd69-78ab-4e96-a426-9b2f7fa91a27",
   "metadata": {},
   "source": [
    "# Log transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f8017-5310-482c-9a46-99185524eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_final_intensity'] = np.log1p(df['final_intensity'])\n",
    "\n",
    "#5 quantiles\n",
    "df['intensity_bin'] = pd.qcut(df['log_final_intensity'], q=5, duplicates='drop')\n",
    "\n",
    "# stratified sample: 20% from each bin\n",
    "df_sample = df.groupby('intensity_bin', group_keys=False).sample(frac=0.2, random_state=42)\n",
    "df_sample = df_sample.drop(columns=['intensity_bin']).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Sampled {len(df_sample):,} rows (stratified by log-transformed final intensity)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2666c4-c66d-4548-80f4-83c271f2fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE HOT ENCODING\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "X = encoder.fit_transform(df_sample[['BB1', 'BB2', 'BB3']])\n",
    "y = df_sample['log_final_intensity']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34aa9949-004a-4c2f-9b06-6378530ca577",
   "metadata": {},
   "source": [
    "## Bayesian optimization using hyperopt (randomized grid searches and normal grid searches took very long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbba0b4-266c-4acd-9e09-7752f281c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "subset_idx = np.random.choice(X_train.shape[0], int(0.3 * X_train.shape[0]), replace=False)\n",
    "\n",
    "X_sub = X_train[subset_idx]\n",
    "y_sub = y_train.iloc[subset_idx.tolist()]  \n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 300, 25),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 10, 1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.1),\n",
    "    'subsample': hp.uniform('subsample', 0.7, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.7, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0, 3),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.5),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 0.5),\n",
    "}\n",
    "\n",
    "# Objective function to minimize (negative MSE)\n",
    "def objective(params):\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=int(params['n_estimators']),\n",
    "        max_depth=int(params['max_depth']),\n",
    "        learning_rate=params['learning_rate'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        gamma=params['gamma'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        n_jobs=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    score = cross_val_score(model, X_sub, y_sub, scoring='neg_mean_squared_error', cv=3).mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "# from numpy.random import default_rng\n",
    "# rstate = default_rng(42)\n",
    "# Run Hyperopt search\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=25,\n",
    "    trials=trials,\n",
    "    rstate = default_rng(42)\n",
    "\n",
    ")\n",
    "\n",
    "print(\"\\nBest hyperparameters found:\")\n",
    "print(best)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2779173-33a7-42ee-b4ba-8f6449f4a6aa",
   "metadata": {},
   "source": [
    "To help the model learn outliers, I applied sample weighting during training. Specifically, I assigned a weight of 5.0 to molecules in the top 5% of final intensity values (81nM − baseline, spatially corrected), and a weight of 1.0 to all others. This weighting scheme encourages the model to prioritize accurate predictions for top performing binders. By increasing the penalty for errors on these rare, high-intensity examples, the model becomes more sensitive to patterns that distinguish strong binders from the rest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62127aca-2b86-49f5-8021-a9588636be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "best_params = {\n",
    "    'colsample_bytree': 0.8897006129500347,\n",
    "    'gamma': 1.2609857524276593,\n",
    "    'learning_rate': 0.0881136665371867,\n",
    "    'max_depth': int(10.0),\n",
    "    'n_estimators': int(275.0),\n",
    "    'reg_alpha': 0.13697324747328013,\n",
    "    'reg_lambda': 0.14798705569405798,\n",
    "    'subsample': 0.9948938425189908,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "\n",
    "threshold = np.percentile(y_train, 95)\n",
    "#give 5× weight to top 5% examples, 1× to the rest\n",
    "sample_weight = np.where(y_train >= threshold, 5.0, 1.0)\n",
    "\n",
    "\n",
    "final_model = XGBRegressor(**best_params)\n",
    "final_model.fit(X_train, y_train,sample_weight=sample_weight)\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nFinal model performance:\")\n",
    "print(f\"Test MSE: {mse:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f197ad2-5c8a-4d3b-8fd9-90dc97add079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(final_model, 'final_xgb_model.pkl')\n",
    "\n",
    "final_model = joblib.load('final_xgb_model.pkl')\n",
    "joblib.dump(encoder, 'final_encoder.pkl')\n",
    "\n",
    "# Later\n",
    "encoder = joblib.load('final_encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e144c0-f900-4e13-9320-66cd124b3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_cutoff = y_test.quantile(0.95)\n",
    "high_idx = y_test >= top_5_cutoff\n",
    "\n",
    "y_pred_high = final_model.predict(X_test[high_idx])\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print(\"Top-5% RMSE:\", mean_squared_error(y_test[high_idx], y_pred_high, squared=False))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c5ce643-9f81-4d6a-be83-3a12b6ef188c",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "The final model achieved a test MSE of 0.863 on log-transformed intensity values, indicating an average prediction error of about 0.93 in log-space. Since the model predicts log(final_intensity + 1), this corresponds to a 2-fold error in the original intensity scale. this result definitely suggetss potential for improvement with model tuning, or maybe pre-processing the features differently. For outliers, the RMSE was even higher at 1.86 in the log space. Overall this model needs a lot more refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3800f4-74e5-42e6-9ef9-12f39bee4a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(y_test[high_idx], y_pred_high, alpha=0.3)\n",
    "plt.xlabel(\"True log intensity (top 5%)\")\n",
    "plt.ylabel(\"Predicted log intensity\")\n",
    "plt.title(\"Predicted vs True for Top 5% Molecules\")\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a72bfc0f-bb5c-48f6-9a97-32b167d814cb",
   "metadata": {},
   "source": [
    "# Model does worse on outliers (top 5%)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cf87237-6676-44ab-99da-76953e4f9f76",
   "metadata": {},
   "source": [
    "## Top 100 Molecules in the dataset itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0748c-b71b-4281-8e5a-9cf0371a462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from IPython.display import display\n",
    "\n",
    "final_model = joblib.load('final_xgb_model.pkl')\n",
    "encoder = joblib.load('final_encoder.pkl')\n",
    "\n",
    "df_sample[\"molecule\"] = df_sample[\"BB1\"] + \"_\" + df_sample[\"BB2\"] + \"_\" + df_sample[\"BB3\"]\n",
    "seen_combos = df_sample[['BB1', 'BB2', 'BB3', 'molecule']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Predict on seen combinations\n",
    "X_seen = encoder.transform(seen_combos[['BB1', 'BB2', 'BB3']])\n",
    "preds = final_model.predict(X_seen)\n",
    "seen_combos[\"predicted_log_intensity\"] = preds\n",
    "\n",
    "top_100_seen = seen_combos.sort_values(\"predicted_log_intensity\", ascending=False).head(100)\n",
    "\n",
    "print(\"Top 100 Seen Molecules by Predicted Intensity:\")\n",
    "display(top_100_seen)\n",
    "\n",
    "top_100_seen.to_csv(\"top_100_seen_molecules.csv\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb53f13a-b745-4f28-8a9b-34b54e996521",
   "metadata": {},
   "source": [
    "## Unseen molecules (new combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c32eb6d-3af6-48d1-a58a-e327bdc558d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import joblib\n",
    "from IPython.display import display\n",
    "\n",
    "final_model = joblib.load('final_xgb_model.pkl')\n",
    "encoder = joblib.load('final_encoder.pkl')\n",
    "\n",
    "# Generate all possible BB1-BB2-BB3 combinations from the full dataset\n",
    "bb1_unique = df['BB1'].unique()\n",
    "bb2_unique = df['BB2'].unique()\n",
    "bb3_unique = df['BB3'].unique()\n",
    "\n",
    "all_combos = pd.DataFrame(\n",
    "    list(itertools.product(bb1_unique, bb2_unique, bb3_unique)),\n",
    "    columns=[\"BB1\", \"BB2\", \"BB3\"]\n",
    ")\n",
    "\n",
    "\n",
    "df_sample[\"molecule\"] = df_sample[\"BB1\"] + \"_\" + df_sample[\"BB2\"] + \"_\" + df_sample[\"BB3\"]\n",
    "seen_molecules = set(df_sample[\"molecule\"])\n",
    "\n",
    "all_combos[\"molecule\"] = (\n",
    "    all_combos[\"BB1\"] + \"_\" +\n",
    "    all_combos[\"BB2\"] + \"_\" +\n",
    "    all_combos[\"BB3\"]\n",
    ")\n",
    "\n",
    "# Filter out seen molecules\n",
    "unseen_combos = all_combos[~all_combos[\"molecule\"].isin(seen_molecules)].reset_index(drop=True)\n",
    "\n",
    "X_unseen = encoder.transform(unseen_combos[['BB1', 'BB2', 'BB3']])\n",
    "unseen_preds = final_model.predict(X_unseen)\n",
    "unseen_combos[\"predicted_log_intensity\"] = unseen_preds\n",
    "\n",
    "top_100_unseen = unseen_combos.sort_values(\"predicted_log_intensity\", ascending=False).head(100)\n",
    "\n",
    "print(\"Top 100 Unseen Molecules by Predicted Intensity:\")\n",
    "display(top_100_unseen)\n",
    "\n",
    "top_100_unseen.to_csv(\"top_100_unseen_molecules.csv\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cf43670-2882-4e4f-8482-317a07b3bb90",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "Upon reviewing the top 100 predicted molecules, the model appears to prioritize similar features to those identified during hit calling. Notably, BB3 components such as CP_80, CP_194, and CP_95 frequently appear among the highest-scoring predictions. This overlap suggests that the model has successfully learned some signal patterns consistent with my hit calling method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f937fd01-8445-47a0-8d01-c09ad267bee1",
   "metadata": {},
   "source": [
    "Below is other scratch work I did not end up using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462fde9-d396-42c4-a59f-3e679058b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Ridge\n",
    "\n",
    "# model = Ridge(alpha=1.0, random_state=42)\n",
    "# model.fit(X_train, y_train)\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "# print(\"R²:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6182f49-4411-4e08-a26b-ffcca6857721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# xgb_model = XGBRegressor(\n",
    "#     n_estimators=200,\n",
    "#     max_depth=12,\n",
    "#     learning_rate=0.1,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# xgb_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a3a90-e74a-48e9-88fb-f4951ccc699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 300, 25),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 10, 1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.1),\n",
    "    'subsample': hp.uniform('subsample', 0.7, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.7, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0, 3),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.5),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 0.5),\n",
    "}\n",
    "\n",
    "# Objective function to minimize (neg MSE)\n",
    "def objective(params):\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=int(params['n_estimators']),\n",
    "        max_depth=int(params['max_depth']),\n",
    "        learning_rate=params['learning_rate'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        gamma=params['gamma'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        n_jobs=1,              y        random_state=42\n",
    "    )\n",
    "    score = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=3).mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=25,             \n",
    "    trials=trials,\n",
    "    rstate=np.random.RandomState(42)\n",
    ")\n",
    "\n",
    "print(\"\\nBest hyperparameters found:\")\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d702613-8599-43eb-91c3-cd10cf373ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"XGBoost MSE: {mse:.2f}\")\n",
    "print(f\"XGBoost R²: {r2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb484c28-0939-4a5b-9d16-f494dd192c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 30% of the training set\n",
    "X_df = pd.DataFrame(X_train)         # in case X_train is still ndarray\n",
    "y_series = pd.Series(y_train)        # same for y_train\n",
    "\n",
    "X_sub = X_df.sample(frac=0.3, random_state=42)\n",
    "y_sub = y_series.iloc[X_sub.index]   # use iloc here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eac1d1-1863-4fbe-a36c-7f2da5e8adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 600),\n",
    "    'max_depth': randint(5, 15),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'gamma': uniform(0, 5),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(0, 1),\n",
    "}\n",
    "\n",
    "\n",
    "xgb = XGBRegressor(random_state=42, n_jobs=1)\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=1 \n",
    ")\n",
    "\n",
    "# Fit\n",
    "# Perform hyperparameter tuning with CV on this subset\n",
    "random_search.fit(X_sub, y_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1085db4e-ef1b-46bc-8645-d848cee86b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred):.4f}\")\n",
    "print(f\"R²: {r2_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc81650d-8f36-4a33-99b3-f2e520117aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest MSE: {mse_rf:.6f}\")\n",
    "print(f\"Random Forest R²: {r2_rf:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb127a2-f4d5-4316-950f-5c5d252e4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "svr_model = make_pipeline(\n",
    "    StandardScaler(with_mean=False),  # don't center sparse matrices\n",
    "    SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    ")\n",
    "\n",
    "svr_model.fit(X_train, y_train)\n",
    "y_pred_svr = svr_model.predict(X_test)\n",
    "\n",
    "\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "print(f\"SVR MSE: {mse_svr:.6f}\")\n",
    "print(f\"SVR R²: {r2_svr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4403d5ec-755c-434c-9b05-119eadaba5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (september)",
   "language": "python",
   "name": "september"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
